'use strict'
/**
 * Generated by Kubb (https://kubb.dev/).
 * Do not edit manually.
 */
Object.defineProperty(exports, '__esModule', { value: true })
exports.responseResourceSchema = void 0
const allowedToolChoiceSchema_ts_1 = require('./allowedToolChoiceSchema.ts')
const errorSchema_ts_1 = require('./errorSchema.ts')
const functionToolChoiceSchema_ts_1 = require('./functionToolChoiceSchema.ts')
const incompleteDetailsSchema_ts_1 = require('./incompleteDetailsSchema.ts')
const itemFieldSchema_ts_1 = require('./itemFieldSchema.ts')
const reasoningSchema_ts_1 = require('./reasoningSchema.ts')
const textFieldSchema_ts_1 = require('./textFieldSchema.ts')
const toolChoiceValueEnumSchema_ts_1 = require('./toolChoiceValueEnumSchema.ts')
const toolSchema_ts_1 = require('./toolSchema.ts')
const truncationEnumSchema_ts_1 = require('./truncationEnumSchema.ts')
const usageSchema_ts_1 = require('./usageSchema.ts')
const zod_1 = require('zod')
/**
 * @description The complete response object that was returned by the Responses API.
 */
exports.responseResourceSchema = zod_1.z
  .object({
    id: zod_1.z
      .string()
      .describe('The unique ID of the response that was created.'),
    object: zod_1.z
      .enum(['response'])
      .default('response')
      .describe('The object type, which was always `response`.'),
    created_at: zod_1.z
      .number()
      .int()
      .describe(
        'The Unix timestamp (in seconds) for when the response was created.',
      ),
    completed_at: zod_1.z.union([zod_1.z.number().int(), zod_1.z.null()]),
    status: zod_1.z
      .string()
      .describe('The status that was set for the response.'),
    incomplete_details: zod_1.z.union([
      zod_1.z
        .lazy(() => incompleteDetailsSchema_ts_1.incompleteDetailsSchema)
        .and(zod_1.z.any()),
      zod_1.z.null(),
    ]),
    model: zod_1.z.string().describe('The model that generated this response.'),
    previous_response_id: zod_1.z.union([zod_1.z.string(), zod_1.z.null()]),
    instructions: zod_1.z.union([zod_1.z.string(), zod_1.z.null()]),
    output: zod_1.z
      .array(
        zod_1.z
          .lazy(() => itemFieldSchema_ts_1.itemFieldSchema)
          .describe(
            'An item representing a message, tool call, tool output, reasoning, or other response element.',
          ),
      )
      .describe('The output items that were generated by the model.'),
    error: zod_1.z.union([
      zod_1.z.lazy(() => errorSchema_ts_1.errorSchema).and(zod_1.z.any()),
      zod_1.z.null(),
    ]),
    tools: zod_1.z
      .array(
        zod_1.z
          .lazy(() => toolSchema_ts_1.toolSchema)
          .describe('A tool that can be used to generate a response.'),
      )
      .describe(
        'The tools that were available to the model during response generation.',
      ),
    tool_choice: zod_1.z.union([
      zod_1.z.lazy(
        () => functionToolChoiceSchema_ts_1.functionToolChoiceSchema,
      ),
      zod_1.z.lazy(
        () => toolChoiceValueEnumSchema_ts_1.toolChoiceValueEnumSchema,
      ),
      zod_1.z.lazy(() => allowedToolChoiceSchema_ts_1.allowedToolChoiceSchema),
    ]),
    truncation: zod_1.z
      .lazy(() => truncationEnumSchema_ts_1.truncationEnumSchema)
      .and(zod_1.z.any()),
    parallel_tool_calls: zod_1.z
      .boolean()
      .describe(
        'Whether the model was allowed to call multiple tools in parallel.',
      ),
    text: zod_1.z
      .lazy(() => textFieldSchema_ts_1.textFieldSchema)
      .and(zod_1.z.any()),
    top_p: zod_1.z
      .number()
      .describe(
        'The nucleus sampling parameter that was used for this response.',
      ),
    presence_penalty: zod_1.z
      .number()
      .describe(
        'The presence penalty that was used to penalize new tokens based on whether they appear in the text so far.',
      ),
    frequency_penalty: zod_1.z
      .number()
      .describe(
        'The frequency penalty that was used to penalize new tokens based on their frequency in the text so far.',
      ),
    top_logprobs: zod_1.z
      .number()
      .int()
      .describe(
        'The number of most likely tokens that were returned at each position, along with their log probabilities.',
      ),
    temperature: zod_1.z
      .number()
      .describe('The sampling temperature that was used for this response.'),
    reasoning: zod_1.z.union([
      zod_1.z
        .lazy(() => reasoningSchema_ts_1.reasoningSchema)
        .and(zod_1.z.any()),
      zod_1.z.null(),
    ]),
    usage: zod_1.z.union([
      zod_1.z.lazy(() => usageSchema_ts_1.usageSchema).and(zod_1.z.any()),
      zod_1.z.null(),
    ]),
    max_output_tokens: zod_1.z.union([zod_1.z.number().int(), zod_1.z.null()]),
    max_tool_calls: zod_1.z.union([zod_1.z.number().int(), zod_1.z.null()]),
    store: zod_1.z
      .boolean()
      .describe(
        'Whether this response was stored so it can be retrieved later.',
      ),
    background: zod_1.z
      .boolean()
      .describe('Whether this request was run in the background.'),
    service_tier: zod_1.z
      .string()
      .describe('The service tier that was used for this response.'),
    metadata: zod_1.z
      .any()
      .describe(
        'Developer-defined metadata that was associated with the response.',
      ),
    safety_identifier: zod_1.z.union([zod_1.z.string(), zod_1.z.null()]),
    prompt_cache_key: zod_1.z.union([zod_1.z.string(), zod_1.z.null()]),
  })
  .describe(
    'The complete response object that was returned by the Responses API.',
  )
