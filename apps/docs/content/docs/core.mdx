---
title: Core Concepts
description: Understanding the Tekimax Client and Provider architecture.
---

# Core Concepts

The Tekimax SDK is built around two main primitives: the `Tekimax` client and the `AIProvider` interface.

## The Client

The `Tekimax` client is the unified entry point. It organizes capabilities into **Namespaces**.

```typescript
const client = new Tekimax({ provider });

// Namespaces
client.text   // Chat, Embeddings
client.images // Generation, Editing, Analysis
client.audio  // Text-to-Speech (TTS)
client.videos // Generation, Analysis
```

### Text (Chat)

The `client.text` namespace provides a standard chat completion interface.

```typescript
const response = await client.text.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'Hello!' }]
});
```

## Providers

Providers act as translation layers. They convert strict Tekimax types into the specific format required by the upstream API (e.g., OpenAI, Anthropic) and normalize the response back.

All providers implement the `AIProvider` interface:

```typescript
interface AIProvider {
  chat(
    request: ChatCompletionRequest
  ): Promise<ChatCompletionResponse>;

  chatStream(
    request: ChatCompletionRequest
  ): AsyncIterable<ChatCompletionChunk>;
}
```

## Streaming

Streaming is handled automatically. If you set `stream: true`, the client returns an AsyncIterable.

```typescript
const stream = await client.text.chat.completions.create({
  model: 'gpt-4o',
  messages: [
    { role: 'user', content: 'Tell me a story' }
  ],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(
    chunk.choices[0]?.delta?.content || ''
  );
}
```

## React Hooks

If you are using React, we provide built-in hooks for instant integration.

```typescript
import { useChat } from 'tekimax-ts/react';
```

See the [React Integration](/docs/guides/react) guide for more details.

## Universal Features

The SDK normalizes advanced features across all providers (OpenAI, Anthropic, Gemini, Ollama), ensuring a consistent API experience.

### Reasoning (Thinking)

Support for "Thinking" models (like DeepSeek R1) is built-in. Use the `think: true` parameter to capture reasoning traces.

```typescript
const response = await client.text.chat.completions.create({
  model: 'deepseek-r1',
  messages: [...],
  think: true
});

console.log(response.choices[0].message.thinking); // Access reasoning
```

See the [Reasoning Models](/docs/guides/thinking) guide for more details.

### Tool Calling

Tool calling is standardized. You define tools once, and the SDK handles the specific format for each provider (e.g., mapping to Gemini's `functionDeclarations` or Anthropic's `tool_use`).

```typescript
const client = new Tekimax({
  provider: new GeminiProvider({ apiKey: '...' }) // Works with Anthropic, OpenAI, etc.
});

const response = await client.text.chat.completions.create({
  model: 'gemini-pro',
  messages: [{ role: 'user', content: 'What is the weather in Tokyo?' }],
  tools: [{
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'Get current weather',
      parameters: {
        type: 'object',
        properties: { location: { type: 'string' } },
        required: ['location']
      }
    }
  }]
});

// Uniform access to tool calls
const toolCalls = response.choices[0].message.tool_calls;
if (toolCalls) {
  console.log(toolCalls[0].function.name); // "get_weather"
}
```
