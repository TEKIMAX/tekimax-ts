---
title: Reasoning Models
description: How to use "Thinking" models like DeepSeek R1 with Tekimax SDK.
---

# Reasoning Models

The Tekimax SDK provides first-class support for **Reasoning Models** (also known as "Thinking" models), such as DeepSeek R1 and newer OpenAI o-series models.

These models expose their internal "chain of thought" process, allowing you to display the reasoning steps to the user before the final answer is generated.

## Enabling Thinking

To enable reasoning, pass `think: true` in your chat options.

```typescript
const response = await client.text.chat.completions.create({
  model: 'deepseek-r1',
  messages: [{ role: 'user', content: 'Solve this logic puzzle...' }],
  think: true // Tells the provider to return reasoning data
});
```

## Accessing Reasoning Data

The reasoning content is available in the `thinking` field of the message.

```typescript
const thinking = response.message.thinking;
const content = response.message.content;

console.log('--- Reasoning ---');
console.log(thinking);
console.log('--- Answer ---');
console.log(content);
```

### Streaming Support

Thinking is also supported in streaming mode. Use `createStream` â€” the `thinking` delta is emitted separately from the content `delta` so you can render them in different UI areas.

```typescript
// createStream returns an AsyncGenerator â€” no await.
const stream = client.text.chat.completions.createStream({
  model: 'deepseek-r1',
  messages: [{ role: 'user', content: 'Solve: 2x + 3 = 11' }],
  think: true,
});

for await (const chunk of stream) {
  // Thinking deltas arrive first, then content deltas.
  if (chunk.thinking) {
    process.stdout.write(`[think] ${chunk.thinking}`);
  }
  if (chunk.delta) {
    process.stdout.write(chunk.delta);
  }
}
```

## Provider Support

The SDK handles three distinct implementation patterns for reasoning, so your code stays the same regardless of provider.

| Provider | Support | How It Works |
| :--- | :--- | :--- |
| **Ollama** | âœ… Native | Ollama returns a dedicated `thinking` field in its response. The SDK passes `think: true` directly to the Ollama API. |
| **OpenAI** | ðŸš§ Model-dependent | Models like `o1` encapsulate reasoning internally. Extraction depends on the model version and API support. |
| **Generic** | ðŸ”„ Emulated | Models that output `<think>` XML tags in their content (like DeepSeek R1 via OpenRouter) are parsed by the SDK using the `parseThinking` utility. |

## Utility: Parsing Raw Thinking

If you're processing raw model output that contains `<think>` tags â€” for example, from an OpenRouter response â€” the SDK provides a `parseThinking` utility that handles three edge cases:

```typescript
import { parseThinking } from 'tekimax-ts';

// Case 1: Complete think block
const complete = parseThinking("<think>Calculating...</think> The answer is 42.");
// { think: "Calculating...", rest: "The answer is 42." }

// Case 2: Streaming â€” tag opened but not closed yet.
// This happens mid-stream when the model is still reasoning.
const streaming = parseThinking("<think>Still working on it...");
// { think: "Still working on it...", rest: "" }

// Case 3: No think tag at all
const none = parseThinking("Just a normal response.");
// { think: null, rest: "Just a normal response." }
```

This utility is used internally by the SDK, but it's exported so you can use it in custom streaming implementations.
